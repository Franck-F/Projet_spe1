#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
EDA France - Analyse et Pr√©diction des Prix de l'√âlectricit√©
=============================================================

Ce script effectue une analyse exploratoire compl√®te des donn√©es de prix de l'√©lectricit√©
en France et construit un mod√®le de pr√©diction utilisant LightGBM avec optimisation Optuna.

Date d'extraction: Novembre 2024
"""

# =============================================================================
# IMPORTS
# =============================================================================

import urllib.request
import os
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import plotly.express as px
import plotly.graph_objects as go
from plotly.subplots import make_subplots
import plotly.figure_factory as ff
from statsmodels.tsa.seasonal import STL
import lightgbm as lgb
from sklearn.metrics import mean_absolute_error, root_mean_squared_error
import optuna
import shap
import seaborn as sns
import skimpy as sk
import summarytools as st
from datetime import datetime
import calendar
import plotly.io as pio

# Configuration des warnings
import warnings
warnings.filterwarnings('ignore')
warnings.simplefilter('ignore', FutureWarning)
warnings.filterwarnings("ignore", category=FutureWarning)
warnings.filterwarnings("ignore", category=optuna.exceptions.ExperimentalWarning)

# Configuration plotly
pio.templates.default = "plotly_white"

print("Environnement configur√© avec succ√®s!")
print(f"Date d'ex√©cution: {datetime.now().strftime('%Y-%m-%d %H:%M')}")

# =============================================================================
# 1. T√âL√âCHARGEMENT ET CHARGEMENT DES DONN√âES
# =============================================================================

# Cr√©er dossier data si inexistant
os.makedirs('../data/raw', exist_ok=True)

# URL dataset 60min
url = "https://data.open-power-system-data.org/time_series/latest/time_series_60min_singleindex.csv"
destination = "../data/raw/time_series_60min.csv"

# T√©l√©charger si pas d√©j√† pr√©sent
if not os.path.exists(destination):
    print("‚è≥ T√©l√©chargement du dataset (124 MB)... Patience!")
    urllib.request.urlretrieve(url, destination)
    print("‚úÖ Dataset t√©l√©charg√©!")
else:
    print("‚úÖ Dataset d√©j√† pr√©sent localement")

# Charger dataset
df = pd.read_csv('../data/raw/time_series_60min.csv',
    parse_dates=['utc_timestamp', 'cet_cest_timestamp'],
    low_memory=False
)

# D√©finir timestamp comme index
df = df.set_index('utc_timestamp')

print(f"üìä Shape du dataset: {df.shape}")
print(f"üìÖ P√©riode: {df.index.min()} ‚Üí {df.index.max()}")
print(f"\nüîç Premi√®res lignes:")
print(df.head(5))

# =============================================================================
# 2. EXTRACTION DES DONN√âES FRANCE
# =============================================================================

# S√©lection des colonnes fran√ßaises
france_cols = [col for col in df.columns if 'FR' in col]
df_france = df[france_cols].copy()

# R√©sum√© du dataset France
print("\n" + "="*60)
print("R√©sum√© du dataset France:")
print("="*60)
print(f"Dimensions: {df_france.shape[0]} lignes √ó {df_france.shape[1]} colonnes")
print(f"P√©riode: {df_france.index.min()} √† {df_france.index.max()}")

# Aper√ßu des premi√®res lignes
print("\nüìä Aper√ßu des donn√©es:")
print(df_france.head())

print("\nüìà Statistiques descriptives:")
print(df_france.describe())

# Analyse avec skimpy et summarytools
sk.skim(df_france)
st.dfSummary(df_france)

# =============================================================================
# 3. V√âRIFICATION DES DOUBLONS
# =============================================================================

print("\n" + "="*60)
print("V√©rification des doublons")
print("="*60)

total = len(df_france)
unique = df_france.index.nunique()
dup = total - unique

print(f"Total lignes: {total}")
print(f"Lignes uniques par utc_timestamp: {unique}")
print(f"Doublons d√©tect√©s: {dup}")

if dup:
    dup_timestamps = df.index[df.index.duplicated(keep=False)].unique()
    print(f"Nombre de timestamps dupliqu√©s uniques: {len(dup_timestamps)}")
    print(pd.DataFrame({"duplicated_timestamp": dup_timestamps}).head(20))
    
    # Afficher un √©chantillon des lignes dupliqu√©es pour inspection
    sample_ts = dup_timestamps[:5]
    for ts in sample_ts:
        print(f"\nExemple pour timestamp dupliqu√©: {ts}")
        print(df_france.loc[ts])
else:
    print("‚úÖ Aucun doublon trouv√© sur utc_timestamp.")

# =============================================================================
# 4. ANALYSE DES VALEURS MANQUANTES
# =============================================================================

print("\n" + "="*60)
print("Analyse des valeurs manquantes")
print("="*60)

# Quantification et visualisation des valeurs manquantes
missing_count = df_france.isna().sum()
missing_pct = (missing_count / len(df_france)) * 100
missing_df_all = (
    pd.DataFrame({"missing_count": missing_count, "missing_pct": missing_pct})
    .sort_values("missing_pct", ascending=False)
)
print(missing_df_all)

# Bar plot des pourcentages de valeurs manquantes
fig_missing_bar = px.bar(
    missing_df_all.reset_index().rename(columns={"index": "column"}),
    x="missing_pct",
    y="column",
    orientation="h",
    text="missing_pct",
    title="Pourcentage de valeurs manquantes par colonne (df_france)",
    labels={"missing_pct": "% NaN", "column": "Colonne"},
)
fig_missing_bar.update_traces(texttemplate="%{text:.2f}%", textposition="outside")
fig_missing_bar.show()

# P√©riodes avec valeurs manquantes pour IT_NORD_FR_price_day_ahead
col = "IT_NORD_FR_price_day_ahead"
mask = df_france[col].isna()

if not mask.any():
    print(f"Aucune valeur manquante pour {col}.")
else:
    # Num√©roter les runs (changes de state)
    run_id = (mask != mask.shift(1)).cumsum()
    runs = (
        df_france[mask]
        .groupby(run_id[mask])
        .apply(lambda x: pd.Series({
            "start": x.index.min(),
            "end": x.index.max(),
            "n_points": len(x)
        }))
        .reset_index(drop=True)
    )
    runs["duration_hours"] = (runs["end"] - runs["start"]) / np.timedelta64(1, "h") + 1
    runs = runs.sort_values("start").reset_index(drop=True)

    print(f"Nombre de p√©riodes disjointes avec des NaN pour {col}: {len(runs)}")
    print(runs)

    overall = pd.Series({
        "first_nan": runs["start"].min(),
        "last_nan": runs["end"].max(),
        "total_nan_points": int(mask.sum()),
        "total_points": len(df_france),
        "nan_pct": mask.mean() * 100
    })
    print(overall)

# =============================================================================
# 5. PR√âTRAITEMENT DES DONN√âES
# =============================================================================

print("\n" + "="*60)
print("Pr√©traitement des donn√©es")
print("="*60)

# Renommer les colonnes pour plus de clart√©
df_france.rename(columns={
    'IT_NORD_FR_price_day_ahead': 'price',
    'FR_load_actual_entsoe_transparency': 'load_actual',
    'FR_load_forecast_entsoe_transparency': 'load_forecast',
    'FR_solar_generation_actual': 'solar_generation',
    'FR_wind_onshore_generation_actual': 'wind_generation'
}, inplace=True)

# Filtrer sur la p√©riode d'int√©r√™t
start_date = '2015-01-05'
end_date = '2017-12-05'
df_france = df_france.loc[start_date:end_date]

print(f"P√©riode filtr√©e: {start_date} ‚Üí {end_date}")
print(f"Nouvelles dimensions: {df_france.shape}")

# V√©rifier les valeurs manquantes apr√®s filtrage
missing_count = df_france.isna().sum()
missing_pct = (missing_count / len(df_france)) * 100
missing_df_all = (
    pd.DataFrame({"missing_count": missing_count, "missing_pct": missing_pct})
    .sort_values("missing_pct", ascending=False)
)
print("\nValeurs manquantes apr√®s filtrage:")
print(missing_df_all)

# Utiliser l'interpolation lin√©aire pour les quelques NaN restants
df_france.interpolate(method='linear', inplace=True)

# V√©rification finale
missing_count = df_france.isna().sum()
missing_pct = (missing_count / len(df_france)) * 100
missing_df_all = (
    pd.DataFrame({"missing_count": missing_count, "missing_pct": missing_pct})
    .sort_values("missing_pct", ascending=False)
)
print("\nValeurs manquantes apr√®s interpolation:")
print(missing_df_all)

print("\n‚úÖ Pr√©traitement termin√©")
print(df_france.head())

# =============================================================================
# 6. ANALYSE EXPLORATOIRE DES DONN√âES (EDA)
# =============================================================================

print("\n" + "="*60)
print("Analyse Exploratoire des Donn√©es")
print("="*60)

# 6.1 Distribution du prix day-ahead
fig = px.histogram(
    df_france,
    x="price",
    nbins=25,
    marginal="rug",           
    opacity=0.75,
    title="Distribution du Day-Ahead Price (‚Ç¨/MWh)",
    labels={"price": "Price (‚Ç¨/MWh)", "count": "Frequency"},
)
fig.update_layout(bargap=0.02, template="plotly_white")
fig.show()

# 6.2 Evolution temporelle du prix
fig = px.line(
    df_france.reset_index(),
    x=df_france.index.name or "index",
    y="price",
    title="Evolution du Day-Ahead Price (2015-2017)",
    labels={
        df_france.index.name or "index": "Date",
        "price": "Price (‚Ç¨/MWh)",
    },
)
fig.update_layout(template="plotly_white")
fig.show()

# 6.3 Analyse de la saisonnalit√©
df_seasonal = df_france.copy()
df_seasonal['month'] = df_seasonal.index.month_name()
df_seasonal['day_of_week'] = df_seasonal.index.day_name()
df_seasonal['hour'] = df_seasonal.index.hour

# Saisonnalit√© annuelle : distribution mensuelle des prix
fig = px.box(
    df_seasonal,
    x="month",
    y="price",
    points="outliers", 
    title="Saisonnalit√© annuelle : distribution mensuelle des prix",
    labels={"month": "Mois", "price": "Prix (‚Ç¨/MWh)"},
    template="plotly_white",
)
fig.update_layout(xaxis=dict(dtick=1))
fig.show()

# Saisonnalit√© hebdomadaire : distribution des prix par jour de la semaine
order_days = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']
fig = px.box(
    df_seasonal,
    x="day_of_week",
    y="price",
    category_orders={"day_of_week": order_days},
    title="Saisonnalit√© hebdomadaire : distribution des prix par jour de la semaine",
    labels={"day_of_week": "Jour de la semaine", "price": "Prix (‚Ç¨/MWh)"},
    template="plotly_white",
)
fig.show()

# Saisonnalit√© journali√®re : prix moyen par heure
hourly_mean = df_seasonal.groupby('hour')['price'].mean().reset_index()
fig = px.line(
    hourly_mean,
    x="hour",
    y="price",
    markers=True,
    title="Saisonnalit√© journali√®re : prix moyen par heure",
    labels={"hour": "Heure", "price": "Prix moyen (‚Ç¨/MWh)"},
    template="plotly_white",
)
fig.update_layout(xaxis=dict(dtick=1))
fig.show()

# Heatmap : prix moyen par jour de la semaine et heure
pivot_day_hour = df_seasonal.pivot_table(
    values='price',
    index='day_of_week',
    columns='hour',
    aggfunc='mean'
)
pivot_day_hour = pivot_day_hour.reindex(order_days)

fig = go.Figure(data=go.Heatmap(
    z=pivot_day_hour.values,
    x=pivot_day_hour.columns,
    y=pivot_day_hour.index,
    colorscale='RdYlBu_r',
    colorbar=dict(title="Prix (‚Ç¨/MWh)"),
))
fig.update_layout(
    title="Prix moyen par jour de la semaine et heure",
    xaxis_title="Heure",
    yaxis_title="Jour de la semaine",
    template="plotly_white",
)
fig.show()

# 6.4 Corr√©lation entre les variables
correlation_matrix = df_france.corr()

fig = go.Figure(data=go.Heatmap(
    z=correlation_matrix.values,
    x=correlation_matrix.columns,
    y=correlation_matrix.index,
    colorscale='RdBu_r',
    zmid=0,
    text=correlation_matrix.values,
    texttemplate='%{text:.2f}',
    colorbar=dict(title="Corr√©lation"),
))
fig.update_layout(
    title="Matrice de corr√©lation",
    template="plotly_white",
    width=800,
    height=700,
)
fig.show()

# 6.5 D√©composition saisonni√®re (STL)
print("\nüîÑ D√©composition saisonni√®re en cours...")

stl = STL(df_france['price'], seasonal=13, period=24*7)
result = stl.fit()

fig = make_subplots(
    rows=4, cols=1,
    subplot_titles=('Prix observ√©', 'Tendance', 'Saisonnalit√©', 'R√©sidu'),
    vertical_spacing=0.05,
)

fig.add_trace(
    go.Scatter(x=df_france.index, y=df_france['price'], mode='lines', name='Observ√©'),
    row=1, col=1
)
fig.add_trace(
    go.Scatter(x=df_france.index, y=result.trend, mode='lines', name='Tendance'),
    row=2, col=1
)
fig.add_trace(
    go.Scatter(x=df_france.index, y=result.seasonal, mode='lines', name='Saisonnalit√©'),
    row=3, col=1
)
fig.add_trace(
    go.Scatter(x=df_france.index, y=result.resid, mode='lines', name='R√©sidu'),
    row=4, col=1
)

fig.update_layout(
    height=900,
    title_text="D√©composition STL du prix de l'√©lectricit√©",
    showlegend=False,
    template="plotly_white",
)
fig.show()

# =============================================================================
# 7. FEATURE ENGINEERING
# =============================================================================

print("\n" + "="*60)
print("Feature Engineering")
print("="*60)

df_featured = df_france.copy()

# 7.1 Features temporelles
df_featured['hour'] = df_featured.index.hour
df_featured['day_of_week'] = df_featured.index.dayofweek
df_featured['day_of_month'] = df_featured.index.day
df_featured['month'] = df_featured.index.month
df_featured['year'] = df_featured.index.year
df_featured['quarter'] = df_featured.index.quarter
df_featured['week_of_year'] = df_featured.index.isocalendar().week.astype(int)
df_featured['is_weekend'] = (df_featured['day_of_week'] >= 5).astype(int)

# 7.2 Features cycliques
df_featured['hour_sin'] = np.sin(2 * np.pi * df_featured['hour'] / 24)
df_featured['hour_cos'] = np.cos(2 * np.pi * df_featured['hour'] / 24)
df_featured['day_sin'] = np.sin(2 * np.pi * df_featured['day_of_week'] / 7)
df_featured['day_cos'] = np.cos(2 * np.pi * df_featured['day_of_week'] / 7)
df_featured['month_sin'] = np.sin(2 * np.pi * df_featured['month'] / 12)
df_featured['month_cos'] = np.cos(2 * np.pi * df_featured['month'] / 12)

# 7.3 Features de lag (d√©calage)
for lag in [1, 2, 3, 24, 48, 168]:
    df_featured[f'price_lag_{lag}'] = df_featured['price'].shift(lag)
    df_featured[f'load_actual_lag_{lag}'] = df_featured['load_actual'].shift(lag)

# 7.4 Moyennes mobiles
for window in [24, 168]:
    df_featured[f'price_rolling_mean_{window}'] = df_featured['price'].rolling(window=window).mean()
    df_featured[f'price_rolling_std_{window}'] = df_featured['price'].rolling(window=window).std()
    df_featured[f'load_actual_rolling_mean_{window}'] = df_featured['load_actual'].rolling(window=window).mean()

# 7.5 Diff√©rences
df_featured['price_diff_1'] = df_featured['price'].diff(1)
df_featured['price_diff_24'] = df_featured['price'].diff(24)

# 7.6 Ratios et interactions
df_featured['load_forecast_error'] = df_featured['load_forecast'] - df_featured['load_actual']
df_featured['renewable_generation'] = df_featured['solar_generation'] + df_featured['wind_generation']
df_featured['renewable_ratio'] = df_featured['renewable_generation'] / (df_featured['load_actual'] + 1)

# Supprimer les lignes avec NaN cr√©√©es par les lags et moyennes mobiles
df_featured = df_featured.dropna()

print(f"‚úÖ Features cr√©√©es")
print(f"Nouvelles dimensions: {df_featured.shape}")
print(f"\nColonnes disponibles:")
print(df_featured.columns.tolist())

# =============================================================================
# 8. PR√âPARATION DES DONN√âES POUR LE MOD√àLE
# =============================================================================

print("\n" + "="*60)
print("Pr√©paration des donn√©es pour le mod√®le")
print("="*60)

# S√©parer features (X) et target (y)
X = df_featured.drop(columns=['price'])
y = df_featured['price']

# Split temporel : 80% train, 20% test
split_date = df_featured.index[int(len(df_featured) * 0.8)]
X_train = X.loc[X.index < split_date]
X_test = X.loc[X.index >= split_date]
y_train = y.loc[y.index < split_date]
y_test = y.loc[y.index >= split_date]

print(f"üìä Ensemble d'entra√Ænement: {X_train.shape}")
print(f"üìä Ensemble de test: {X_test.shape}")
print(f"üìÖ Date de split: {split_date}")

# =============================================================================
# 9. MOD√àLE DE BASE (LIGHTGBM SANS OPTIMISATION)
# =============================================================================

print("\n" + "="*60)
print("Entra√Ænement du mod√®le LightGBM de base")
print("="*60)

# Entra√Ænement du mod√®le
model = lgb.LGBMRegressor(random_state=42)
print("\n‚è≥ Entra√Ænement en cours...")
model.fit(X_train, y_train)
print("‚úÖ Entra√Ænement termin√©")

# Pr√©dictions
y_pred = model.predict(X_test)

# Evaluation
mae = mean_absolute_error(y_test, y_pred)
rmse = root_mean_squared_error(y_test, y_pred)

print("\n" + "="*60)
print("Performance du mod√®le de base")
print("="*60)
print(f"Mean Absolute Error (MAE): {mae:.2f} ‚Ç¨/MWh")
print(f"Root Mean Squared Error (RMSE): {rmse:.2f} ‚Ç¨/MWh")

# Visualisation
fig = go.Figure()
fig.add_trace(
    go.Scatter(
        x=y_test.index,
        y=y_test,
        mode="lines",
        name="Actual Price",
        line=dict(color="royalblue", width=2),
        opacity=0.8,
    )
)
fig.add_trace(
    go.Scatter(
        x=y_test.index,
        y=y_pred,
        mode="lines",
        name="Predicted Price",
        line=dict(color="firebrick", width=2, dash="dash"),
    )
)
fig.update_layout(
    title="Actual vs. Predicted Prices (Test Set) - Mod√®le de base",
    xaxis_title="Date",
    yaxis_title="Price (‚Ç¨/MWh)",
    template="plotly_white",
    hovermode="x unified",
    legend=dict(orientation="h", yanchor="bottom", y=1.02, xanchor="right", x=1),
)
fig.show()

# =============================================================================
# 10. OPTIMISATION DES HYPERPARAM√àTRES AVEC OPTUNA
# =============================================================================

print("\n" + "="*60)
print("Optimisation des hyperparam√®tres avec Optuna")
print("="*60)

def objective(trial, X_train, y_train, X_val, y_val):
    """Fonction objectif pour Optuna"""
    params = {
        "objective": "regression_l1",
        "metric": "mae",
        "n_estimators": trial.suggest_int("n_estimators", 100, 1000),
        "learning_rate": trial.suggest_float("learning_rate", 0.01, 0.3),
        "num_leaves": trial.suggest_int("num_leaves", 20, 300),
        "max_depth": trial.suggest_int("max_depth", 3, 12),
        "min_child_samples": trial.suggest_int("min_child_samples", 5, 100),
        "feature_fraction": trial.suggest_float("feature_fraction", 0.5, 1.0),
        "bagging_fraction": trial.suggest_float("bagging_fraction", 0.5, 1.0),
        "bagging_freq": trial.suggest_int("bagging_freq", 1, 7),
        "lambda_l1": trial.suggest_float("lambda_l1", 1e-8, 10.0, log=True),
        "lambda_l2": trial.suggest_float("lambda_l2", 1e-8, 10.0, log=True),
        "random_state": 42,
        "verbose": -1,
    }
    
    # Entra√Ænement
    model = lgb.LGBMRegressor(**params)
    model.fit(X_train, y_train)
    
    # Evaluation
    preds = model.predict(X_val)
    mae = mean_absolute_error(y_val, preds)
    
    return mae

# Split Train / Validation / Test
# Test sur les 3 derniers mois
test_start_date = df_featured.index.max() - pd.DateOffset(months=3)
# Validation sur les 3 mois avant le test
val_start_date = test_start_date - pd.DateOffset(months=3)

X_train_full = X.loc[X.index < val_start_date]
y_train_full = y.loc[y.index < val_start_date]
X_val = X.loc[(X.index >= val_start_date) & (X.index < test_start_date)]
y_val = y.loc[(y.index >= val_start_date) & (y.index < test_start_date)]
X_test = X.loc[X.index >= test_start_date]
y_test = y.loc[y.index >= test_start_date]

print(f"Ensemble d'entra√Ænement: {X_train_full.shape}")
print(f"Ensemble de validation: {X_val.shape}")
print(f"Ensemble de test: {X_test.shape}\n")

# Lancement de l'optimisation
print("üîß D√©but du r√©glage d'hyperparam√®tres avec Optuna...")
print("(Cela peut prendre plusieurs minutes)")

study = optuna.create_study(direction='minimize')
study.optimize(
    lambda trial: objective(trial, X_train_full, y_train_full, X_val, y_val), 
    n_trials=50
)

print("\n‚úÖ R√©glage termin√©!")
print(f"Meilleur MAE: {study.best_value:.4f} ‚Ç¨/MWh")
print("\nMeilleurs hyperparam√®tres:")
for key, value in study.best_params.items():
    print(f"  {key}: {value}")

# =============================================================================
# 11. ENTRA√éNEMENT DU MOD√àLE FINAL
# =============================================================================

print("\n" + "="*60)
print("Entra√Ænement du mod√®le final")
print("="*60)

# R√©entra√Ænement sur l'ensemble complet (train + validation)
X_train_final = pd.concat([X_train_full, X_val])
y_train_final = pd.concat([y_train_full, y_val])

best_params = study.best_params
final_model = lgb.LGBMRegressor(**best_params, random_state=42)

print("‚è≥ Entra√Ænement en cours...")
final_model.fit(X_train_final, y_train_final)
print("‚úÖ Entra√Ænement termin√©")

# Evaluation finale
final_preds = final_model.predict(X_test)
final_mae = mean_absolute_error(y_test, final_preds)
final_rmse = root_mean_squared_error(y_test, final_preds)

print("\n" + "="*60)
print("Performance finale du mod√®le sur l'ensemble de test")
print("="*60)
print(f"Mean Absolute Error (MAE): {final_mae:.2f} ‚Ç¨/MWh")
print(f"Root Mean Squared Error (RMSE): {final_rmse:.2f} ‚Ç¨/MWh")

# Visualisation finale
fig = go.Figure()
fig.add_trace(
    go.Scatter(
        x=y_test.index,
        y=y_test,
        mode="lines",
        name="Actual Price",
        line=dict(color="royalblue", width=2),
        opacity=0.8,
    )
)
fig.add_trace(
    go.Scatter(
        x=y_test.index,
        y=final_preds,
        mode="lines",
        name="Predicted Price",
        line=dict(color="firebrick", width=2, dash="dash"),
    )
)
fig.update_layout(
    title="Actual vs Predicted Prices (Test Set) - Mod√®le Final Optimis√©",
    xaxis_title="Date",
    yaxis_title="Price (‚Ç¨/MWh)",
    template="plotly_white",
    hovermode="x unified",
    legend=dict(orientation="h", yanchor="bottom", y=1.02, xanchor="right", x=1),
)
fig.show()

# =============================================================================
# 12. ANALYSE SHAP (EXPLICABILIT√â)
# =============================================================================

print("\n" + "="*60)
print("Analyse SHAP - Explicabilit√© du mod√®le")
print("="*60)

# R√©entra√Ænement sur l'ensemble complet pour SHAP
# (Utilisation des meilleurs param√®tres trouv√©s par Optuna)
best_params_shap = {
    'n_estimators': 965,
    'learning_rate': 0.023305698375173753,
    'num_leaves': 62, 
    'max_depth': 6, 
    'min_child_samples': 73, 
    'feature_fraction': 0.9107029697230191, 
    'bagging_fraction': 0.684479044902335, 
    'bagging_freq': 3, 
    'lambda_l1': 0.029500110573817455, 
    'lambda_l2': 0.16428913564785613,
    'random_state': 42
}

print("\n‚è≥ Entra√Ænement du mod√®le complet pour SHAP...")
shap_model = lgb.LGBMRegressor(**best_params_shap)
shap_model.fit(X, y)
print("‚úÖ Entra√Ænement termin√©")

# Calcul des valeurs SHAP
test_start_date = df_featured.index.max() - pd.DateOffset(months=3)
X_test_shap = X.loc[X.index >= test_start_date]

print("\n‚è≥ Calcul des valeurs SHAP...")
explainer = shap.TreeExplainer(shap_model)
shap_values = explainer.shap_values(X_test_shap)
print("‚úÖ Valeurs SHAP calcul√©es")

# Visualisation 1: Importance globale des features
if isinstance(shap_values, list):
    shap_array = shap_values[0]
else:
    shap_array = shap_values

shap_importance = (
    pd.DataFrame({
        "feature": X_test_shap.columns,
        "mean_abs_shap": np.abs(shap_array).mean(axis=0),
    })
    .sort_values("mean_abs_shap", ascending=True)
    .reset_index(drop=True)
)

fig = px.bar(
    shap_importance,
    x="mean_abs_shap",
    y="feature",
    orientation="h",
    title="Importance globale des caract√©ristiques (|SHAP| moyen)",
    labels={"mean_abs_shap": "|SHAP| moyen", "feature": "Caract√©ristique"},
    template="plotly_white",
)
fig.update_layout(margin=dict(l=120, r=40, t=60, b=40))
fig.show()

# Visualisation 2: SHAP Summary Plot (Beeswarm)
index_name = X_test_shap.index.name or "index"
shap_frame = pd.DataFrame(shap_array, columns=X_test_shap.columns)
shap_frame[index_name] = X_test_shap.index
shap_long = shap_frame.melt(id_vars=index_name, var_name="feature", value_name="shap_value")

feature_long = (
    X_test_shap.reset_index()
    .melt(id_vars=index_name, var_name="feature", value_name="feature_value")
)

merged = shap_long.merge(feature_long, on=[index_name, "feature"])

fig = px.strip(
    merged,
    x="shap_value",
    y="feature",
    color="feature_value",
    title="SHAP Summary Plot (Beeswarm) - Distribution et impact des caract√©ristiques",
    labels={
        "shap_value": "Valeur SHAP", 
        "feature": "Caract√©ristique", 
        "feature_value": "Valeur de la caract√©ristique"
    },
    template="plotly_white",
    orientation="h",
    width=900,
    height=600,
)
fig.update_traces(opacity=0.6, jitter=0.35, marker=dict(symbol="circle", size=6))
fig.update_layout(margin=dict(l=140, r=40, t=60, b=40))
fig.show()

# =============================================================================
# FIN DU SCRIPT
# =============================================================================

print("\n" + "="*60)
print("‚úÖ Analyse termin√©e avec succ√®s!")
print("="*60)
print(f"\nR√©sum√© des r√©sultats:")
print(f"  - MAE final: {final_mae:.2f} ‚Ç¨/MWh")
print(f"  - RMSE final: {final_rmse:.2f} ‚Ç¨/MWh")
print(f"  - Nombre de features: {X.shape[1]}")
print(f"  - P√©riode de test: {X_test.index.min()} ‚Üí {X_test.index.max()}")
print(f"\nüìä Graphiques g√©n√©r√©s et mod√®le entra√Æn√© avec succ√®s!")
